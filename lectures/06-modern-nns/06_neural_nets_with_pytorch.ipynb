{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Networks with PyTorch\n",
    "Based on the notebook of the same name by [Aurelien Geron](https://github.com/ageron/handson-mlp/blob/main/10_neural_nets_with_pytorch.ipynb).\n",
    "\n",
    "## Objectives\n",
    "- Understand the basic structure of a neural network\n",
    "- Understand how to work with PyTorch\n",
    "- Build and train a simple classifier\n",
    "- Explore some more features of the PyTorch ecosystem\n",
    "\n",
    "First we'll load the necessary libraries and make sure we have the right versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Python version must be at least 3.10\n",
    "assert sys.version_info >= (3, 10)\n",
    "from packaging.version import Version\n",
    "import sklearn\n",
    "\n",
    "assert Version(sklearn.__version__) >= Version(\"1.6.1\")\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "\n",
    "assert Version(torch.__version__) >= Version(\"2.6.0\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Just in case there's some kind of hardware acceleration\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good old California housing datasets\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's fundamental datatype is a Tensor - a GPU capable 32-bit float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_valid = torch.FloatTensor(X_valid)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "means = X_train.mean(dim=0, keepdims=True)\n",
    "stds = X_train.std(dim=0, keepdims=True)\n",
    "X_train = (X_train - means) / stds\n",
    "X_valid = (X_valid - means) / stds\n",
    "X_test = (X_test - means) / stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch expects the targets to have one row per sample, so let's reshape the targets to be column vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "y_valid = torch.FloatTensor(y_valid).view(-1, 1)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "n_features = X_train.shape[1]  # there are 8 input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Regression MLP\n",
    "PyTorch doesn't provide a training loop by default, so we need to define it. There are higher-level wrappers that do this kind of thing for you(e.g. Keras, PyTorch Lightning) but it's nice to diy for learning and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bgd(model, optimizer, criterion, X_train, y_train, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward() # calculate the gradients wrt loss -> this is backprop!\n",
    "        optimizer.step() # take a step down the gradient\n",
    "        optimizer.zero_grad() # reset the gradients to zero\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "n_epochs = 20\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "mse = nn.MSELoss()\n",
    "train_bgd(model, optimizer, mse, X_train, y_train, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Mini-Batch Gradient Descent using DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – build the model just like earlier\n",
    "torch.manual_seed(42)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 50), nn.ReLU(),\n",
    "    nn.Linear(50, 40), nn.ReLU(),\n",
    "    nn.Linear(40, 1)\n",
    ")\n",
    "\n",
    "# If we have a GPU, we need to explicitly move the model to it\n",
    "model = model.to(device)\n",
    "\n",
    "# extra code – build the optimizer and loss function, as earlier\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # We also need to move each batch of data to the GPU\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {mean_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, mse, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, metric_fn, aggregate_fn=torch.mean):\n",
    "    model.eval()\n",
    "    metrics = []\n",
    "    # disable autograd calculation for inference time\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric = metric_fn(y_pred, y_batch)\n",
    "            metrics.append(metric)\n",
    "    return aggregate_fn(torch.stack(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "valid_mse = evaluate(model, valid_loader, mse)\n",
    "valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_pred, y_true):\n",
    "    return ((y_pred - y_true) ** 2).mean().sqrt()\n",
    "\n",
    "evaluate(model, valid_loader, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mse.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, valid_loader, mse,\n",
    "         aggregate_fn=lambda metrics: torch.sqrt(torch.mean(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "def evaluate_tm(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()  # reset the metric at the beginning\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            metric.update(y_pred, y_batch)  # update it at each iteration\n",
    "    return metric.compute()  # compute the final result at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "evaluate_tm(model, valid_loader, rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(model, optimizer, criterion, metric, train_loader, valid_loader,\n",
    "               n_epochs):\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        metric.reset()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            model.train()\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            metric.update(y_pred, y_batch)\n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        history[\"train_losses\"].append(mean_loss)\n",
    "        history[\"train_metrics\"].append(metric.compute().item())\n",
    "        history[\"valid_metrics\"].append(\n",
    "            evaluate_tm(model, valid_loader, metric).item())\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, \"\n",
    "              f\"train loss: {history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train metric: {history['train_metrics'][-1]:.4f}, \"\n",
    "              f\"valid metric: {history['valid_metrics'][-1]:.4f}\")\n",
    "    return history\n",
    "\n",
    "torch.manual_seed(42)\n",
    "learning_rate = 0.01\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 50), nn.ReLU(),\n",
    "    nn.Linear(50, 40), nn.ReLU(),\n",
    "    nn.Linear(40, 30), nn.ReLU(),\n",
    "    nn.Linear(30, 1)\n",
    ")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)\n",
    "mse = nn.MSELoss()\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "history = train2(model, optimizer, mse, rmse, train_loader, valid_loader,\n",
    "                 n_epochs)\n",
    "\n",
    "# Since we compute the training metric\n",
    "plt.plot(np.arange(n_epochs) + 0.5, history[\"train_metrics\"], \".--\",\n",
    "         label=\"Training\")\n",
    "plt.plot(np.arange(n_epochs) + 1.0, history[\"valid_metrics\"], \".-\",\n",
    "         label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.title(\"Learning curves\")\n",
    "plt.axis([0.5, 20, 0.4, 1.0])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
